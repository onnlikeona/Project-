---
title: "Application of Methods"
author: "Onalerona Letebele"
date: "24/09/2020"
output:
  html_document: default
  pdf_document: default
toc: yes
---
```{r echo=FALSE, eval=TRUE, message=FALSE}

library('depmixS4')
library('quantmod')
library('knitr')
set.seed(1)
```

# Introduction

This short report will detail how the Hidden Markov Model is used to detect regime shifts in the financial market. This report will not cover the mathematical formulation behind the Hidden Markov Model but rather the focus will be attempting to show how the model performs when applied to simulated and real financial data. R will be used to achieve this and snippets of the code will be provided in-text to elucidate the how the method works. Our learnings from this report will then be formally contextualized in our final paper. 

We will begin by giving a brief description of the Hidden Markov Model (HMM) and how it relates to the detection of financial market regime shifts in asset returns. This will then be followed by the application of the model on simulated nd the S&P500 returns data. Lastly, we will conclude on the model's perfomance in regime detection by comparing its results to the two forms of data as well as the alternative method, that being STARS.


# Hidden Markov Models and Market Regimes

Asset returns in financial markets are often subject to abrupt changes that do not display an observable process. The asset returns then remain in this transitioned state
for a considerable period of time before it's subject to another abrupt change. These persistent periods of time are referred to as market regimes. And the visible abrupt changes are in fact a result of a hidden or latent process. This hidden process is generated by another set of observations that are incomplete but that can be known. For example, changes in national or global policy. However, it is difficult to track these changes consistently and therefore a tool is needed to detect these shifts. This is where the Hidden Markov Model finds use. 

The Hidden Markov Model is based on the concept of a Markov Model. A Markov Model models the probability of moving from one state to another given the current state and disregarding the previous states. This is knwon as the Markov property and is said to be 'memoryless'. The Hidden Markov Model will link the observations to the underlying states under the Markov property. In our context, the observations are the asset returns which are visible and the states are the market regimes which are hidden. The figure below illustrates this with the orange bubbles being the states (market regimes) and the blue bubbles being the observations (asset returns):

```{r echo=FALSE}
include_graphics("C:/Users/student/Downloads/qs-hmm-state-model.png")

```


The observations (blue bubbles) are as a result of the hidden process that's occuring between the states (orange bubbles) as shown by the arrows. What we want the model to achieve is to indicate to us when the transition from one orange bubble to another actually happened. This will help in giving us insight on why an observation abruptly changed. The model can also help in indicating impending shifts, but this is beyond the scope of our project. 

The Hidden Markov Model will give the probaibility of being in a particular market regime given the sequence of asset returns (i.e. the posterior probability). Let's say for instance there exists two market regime states. The model will simply give us the posterior probability of either being in regime 1 or regime 2. Now if the model displays a posterior probability equal to 1 of being in regime 1 and a posterior probability of 0 of being in regime 2 and this now switches to a posterior probability of 0 and 1 respectively, then the time point at which that switch happened is where a regime shift had occured. 

## Simulated Data 

We will now apply the Hidden Markov Model to a simulated dataset containing five separate market regimes each of which will be either a bullish or bearish market regime. The simulated asset returns will be generated from a normal distribution with parameters that correspond to the underlying market regime, each of which will either be a bullish or bearish market regime. In a bullish market, it is expected that the value of asset returns will rise. As such, we will assume that asset returns that fall under this regime will be normally distributed with a positive mean and low variance. Conversely, in a bearish market, asset returns are expected to decrease and we will assume that asset returns under this regime will be normally distributed with a negative mean and high variance. The five market regimes will form a single time path. The use of the model will then be to detect where the market regime shifts from bullish to bearish and vice versa.

### Model Fitting

Two market regimes will initally be simultaed. Each kth regime will have N_k days of the corresponding asset returns. In our example, we will let k = 5 and N_k be between 50 and 150 days. Asset returns under the bull market will be distributed as N(0.1, 0.1) while those of the bear market will be distributed as N(-0.05, 0.2). The parameters are assigned as follows in the code:

```{r}

# Parameters for the bull and bear market returns
Nk_lower <- 50
Nk_upper <- 150
bull_mean <- 0.1
bull_var <- 0.1
bear_mean <- -0.05
bear_var <- 0.2

```

The N_k values are then randomly chosen:

```{r message=FALSE}
# Number of days for each market regime
days <- replicate(5, sample(Nk_lower:Nk_upper, 1))
```
The asset returns corresponding to each of the kth regimes are randomly generated:

```{r message=FALSE}
# Bull and bear markets returns
market_bull_1 <- rnorm( days[1], bull_mean, bull_var ) 
market_bear_2 <- rnorm( days[2], bear_mean, bear_var ) 
market_bull_3 <- rnorm( days[3], bull_mean, bull_var ) 
market_bear_4 <- rnorm( days[4], bear_mean, bear_var ) 
market_bull_5 <- rnorm( days[5], bull_mean, bull_var )
```
We will now create true regime states and the final list  of asset returns, assigning a value of 1 for bullish market regimes and 2 for bearish ones:

```{r message=FALSE}
# List of true regime states and full returns list
true_regimes <- c( rep(1,days[1]), rep(2,days[2]), rep(1,days[3]), rep(2,days[4]), rep(1,days[5]))
returns <- c( market_bull_1, market_bear_2, market_bull_3, market_bear_4, market_bull_5)
```
If we plot the returns, we can clearly see the changes in mean and variance between the regime shifts:

```{r message=FALSE}
plot(returns, type="l", xlab='', ylab="Returns") 
```
At this stage, we can now fit the model with two market regimes:

```{r message=FALSE}
# Fit the Hidden Markov Model
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 2, data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
```
Upon fitting the model, it is now possible to plot the probabilities of being in a particular regime stae given the asset returns (i.e. the posterior probabilities):

```{r message=FALSE}
# The true regimes and the posterior probabilities of the regimes
post_probs <- posterior(hmmfit)
layout(1:2)
plot(post_probs$state, type='s', main='True Regimes', xlab='', ylab='Regime')
matplot(post_probs[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
legend(x='topright', c('Bull','Bear'), fill=1:2, bty='n')
```
The figure above shows that the model peforms well in correctly identifying the market regimes although not perfectly.



## Real Data

Unlike the simulated data set we don't know the number of regimes states that exist beforehand as well as the distribution family our data belongs to. This is a unsupervised learning challenge since the number of states is not known as the data is not labeled. For our initial estimate of the number of regime states, we need to consider the assest class, trading manner for the assest and the time period chosen. 

We will be using the S&P500 returns from 2004 onwards. This data set is from the quantmod library.  


```{r Data, echo=TRUE,eval=TRUE,message=FALSE}
getSymbols( "^GSPC", from="2004-01-01" )
Rets = diff( log( Cl( GSPC ) ) )
returns = as.numeric(Rets)
```

### Plotting our Time Series

```{r Plot, echo=TRUE,eval=TRUE, message=FALSE}
plot(Rets)
```

From the plot above we can see that the time series experiences some sharp and large deviations at different times. The time series seems to be centered around zero. The variance seems to fluctuate around 2008, 2010, 2011 and 2020. Next, we want to identify all the switches between the regime states.

# Now we want to identify all the switches between regime states.

### Fitting  Hidden Markov Model with 3 Regime states

At this stage we will make an initial estimate, there exists two regime states, "bullish" and "bearish" states. These two states represent the performance/behaviour of the returns. An HMM can be fitted using the Expectation Maximisation algorithm, the number of states is set to two and family of distributions set as gaussian. We consider the posterior probabilities of being in a particular regime state. We compare compared the posterior probabilities with the returns.

```{r Fitting,eval=TRUE,echo=TRUE, message=FALSE}
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 2, data=data.frame(returns=returns))
hmm_fit <- fit(hmm, verbose = FALSE)
post_probs <- posterior(hmm_fit)
layout(1:2)
plot(Rets)
matplot(post_probs[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
legend(x='bottomleft', c('Regime #1','Regime #2'), fill=1:2, bty='n')
```
The Hidden markov model split the time series into two regime and identified where each regime exists. Before 2007 the returns were calmer and hence the Hidden Markov Model has given high posterior probability to Regime #1 for this period. However between 2007-2009 the returns were incredibly volatile due to the sub-prime crisis. The returns became calmer in 2010 but additional volatility occurred in 2011.


### Fitting  Hidden Markov Model with 3 Regime states

# Lets consider fitting the Hidden Markov Model but now setting the number of regime states to three and analyse the results.

```{r Fitting3 ,eval=TRUE,echo=TRUE, message=FALSE}
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 3, data=data.frame(returns=returns))
hmm_fit <- fit(hmm, verbose = FALSE)
post_probs <- posterior(hmm_fit)
layout(1:2)
plot(Rets)
matplot(post_probs[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
legend(x='bottomleft', c('Regime #1','Regime #2', 'Regime #3'), fill=1:3, bty='n')
```

The plot is tricky to read but we can see that the regime exists very close to each other. Since the model considers three regimes states, this leads to a switching behaviour between regimes. Choosing the initial number of states to apply is a challenging task, so we need to do data exporation before fiting the model to produce better results.

# STARS Method

We will now look at the STARS method to detect market regime shifts. For this report we are going to consider a shift in the mean as outlined by Rodionov. A very brief summary of how the method works follows. We firstly calculate the mean for for a particular cutoff period. The cutoff is the period of time that we believe should constitute a regime, according to the context. We then recalculate the mean but with the next observation in our time series included. If the inclusion of this observation changes the previous mean siginificantly, that time point (eg. t2) is flagged as a potential regime shift. To confirm that this time point is indeed indicative of a regime shift, we calculate the RSI for each observation following our potential time point (t2). If the RSI changes signs for any subsequent observations (eg. if the RSI for t2 is 0.5 and for t3 -0.5) within a certain cutoff, then the potential time point (t2) is not considered to be indicative of a regime shift.

The cutoff is the only parameter that we need to define for our example. The default pvalue for the t-test is 0.05. To  make the plots easier to analyse, the RSI plot only displays the RSI values of potential time points. It will not diplay the RSI values that are calculated after a potential time point. RSI values of potential time points that later turned out to be not be regime shift timepoint (due to the RSI changing signs down the line, within the cutoff period) will also not be displayed. As such, the number of RSI bars indicating the RSI values will be equal to the number of regime shifts the model has predicted.

## Simulated Data

We start off by simulating our data:
```{r Simulated,eval=TRUE,echo=FALSE, message=FALSE}
days <- replicate(5, sample(50:100,1))
market_bull_1 <- rnorm( days[1], 0.1,0.1) 
market_bear_2 <- rnorm( days[2], -0.05,0.2) 
market_bull_3 <- rnorm( days[3],0.1,0.1) 
market_bear_4 <- rnorm( days[4],-0.05,0.2) 
market_bull_5 <- rnorm( days[5],0.1,0.1)
# Create the list of true regime states and full returns list
true_regimes <- c( rep(1,days[1]), rep(2,days[2]), rep(1,days[3]), rep(2,days[4]), rep(1,days[5]))
returns <- c( market_bull_1, market_bear_2, market_bull_3, market_bear_4, market_bull_5)
returns <- as.vector(returns)
names(returns) <- as.character(1900:(1900+(length(returns)-1)))
plot(returns, type="l", ylab="Returns")
```
We simulate market returns that switch between a "bullish" regime state to a "bearish" regime state as before with HMM. Therefore there are only two regime states in our data. Our returns in the "bullish" regime state are from a normal distribution with a mean of 0.1 and a varuance of 0.1. Our returns in the "bearish" regime state are from a normal distribution with a mean of -0.05 and a varuance of 0.2. We would like the STARS function to detect when the returns switch between regime states.

```{r echo=FALSE,eval=TRUE, message=FALSE}
stars<- function(y=c(rnorm(50), rnorm(50,2,1)), L=20, p=0.05, h=1,  AR1red="none", prewhitening = F) {
  
  
  if(AR1red=="none" && prewhitening== T) stop("Impossible to perform prewhitening if AR1 coefficient is not estimated")
  
  m = round((L+1)/3)				# formula to estimate subsample size for calculating alpha (Rodionov 2006 + http://www.climatelogic.com/documentation/red-noise-estimation)    	  
  
  # library("MASS") needed if you want to use Huber correction parameter built in to MASS   
  
  
  
  
  
  # -------------------------------------------------------------------------
  #    hWeightedAverage(xwin)
  
  #     Calculates the mean estimate for a given range using Huber's weights.
  # -------------------------------------------------------------------------
  
  
  hWeightedAverage<-function(xwin, h){
    
    # simple estimate of the regime mean for the windowed clip
    dblEstAve <- mean(xwin);
    
    for(jjj in 1:2){
      sumWeights = 0
      sumAve = 0
      
      # Estimate normalised deviation
      xDev = (xwin-dblEstAve)/sqrt(sigL)
      
      # Estimate weights or normalised deviation
      xDev[xDev==0] = 1
      wDev = pmin(rep(1, length(xwin)), h/abs(xDev), na.rm=T)
      
      #sum weights and weighed values
      sumWeights = sum(wDev)
      sumAve = sum(xDev*wDev)
      
      sumAve = sumAve/sumWeights
      sumAve = sumAve*sqrt(sigL) + dblEstAve
      dblEstAve = sumAve
    }
    
    dblWeightedAve = dblEstAve
    # hestimate<- huber(xwin, h)
    # dblWeightedAve = hestimate$mu
  }
  
  
  
  #-------------------------------------------------------------------
  # estimateSigma
  # Estimate the long-term, L-pt variance (assume homoskedastic signals).
  #-------------------------------------------------------------------
  
  estimateSigma<-function(x, L){
    
    # Estimate the long-term length-L variance. If the signal >> length of the analysis window, sample to estimate the variance.
    
    nx<-length(x)
    if(nx/L>300) ix <- as.integer(runif(100)*(nx-2*L)+L) else ix<-seq(L,nx,1)
    s<-0
    for(i in 1:length(ix)){
      xwin <- x[(ix[i]-L+1):ix[i]]
      s <- s + var(xwin, na.rm=T)
    }
    sigL1 = s / length(ix)
    sigL1
  }
  
  
  # ------------------------------------------------------------------
  #   getThreshold()
  #
  #   Calculate the critical threshold of deviation that signals regime changes.  This does not change over the signal.
  # ---------------------------------------------------------------
  getThreshold<-function(L, p, sigL){
    
    if(prewhitening == T){
      dof <- 2*L-2						# number degrees freedom
    } else {
      dof <- EqN((2*L-2), alpha)
    }
    
    t <- abs(qt(p/2, dof));              # crit 2-sided t-value
    thresh1 = t*sqrt(2*sigL/L);          # crit deviation
    thresh1
  }
  
  
  
  # -------------------------------------------------------------------
  
  # OLS estimate of AR1 coefficient from Orcutt and Winokur, 1969, Econometrica, 37:1,1-14
  
  # -------------------------------------------------------------------
  
  OLScalc<-function(x){
    Nobs = length(x)
    
    ave1 = mean(x[2:Nobs])
    ave2 = mean(x[1:(Nobs-1)])
    
    sumNom=0
    sumDenom=0
    for(i in 2:Nobs){
      sumNom = sumNom + (x[i] - ave1) * (x[i - 1] - ave2)
      sumDenom = sumDenom + (x[i - 1] - ave2) * (x[i - 1] - ave2)
    }
    if(sumDenom > 0) OLSAR1 = sumNom / sumDenom else OLSAR1 = 0
    OLSAR1
  }
  
  
  # -------------------------------------------------------------------
  # AR1 correlation estimate (alpha)
  
  # ------------------------------------------------------------------- 
  
  AR1cor<-function (mxm, y){
    mm = mxm #define this in big function above
    ny=length(y)
    iy=seq(from = 1,to= ny - (mm+1))
    OLS=rep(NA, length(iy))
    
    # Calculate OLS for sequential samples of length m
    for(i in 1:length(iy)){
      
      xwin = y[(iy[i]):(iy[i]+(mm-1))]
      
      if(length(xwin[is.na(xwin)]) == 0)   OLS[i] <- OLScalc(xwin)
    }
    
    est<-median(OLS, na.rm=T)
    
    # Calculate IP4	
    IP4= est + 1/mm
    for(j in 1:3) IP4=IP4 + abs(IP4)/mm
    
    # Calculate MPK
    if (mm>4) MPK=((mm-1)*est+1)/(mm-4) else MPK= est
    
    alphaEst<-c(est, MPK, IP4) 	
    
  } 	
  
  
  # -------------------------------------------------------------------
  # Function EqP: calculates t-test using equivalent sample size as in von Storch and Zwiers (1999, p.115)
  # ------------------------------------------------------------------- 
  
  EqP= function(rng1, rng2){   
    
    # Set standard no-result for if command at end
    EqP = 0
    
    # Calculate means and variances
    ave1 = mean(rng1, na.rm =T)
    ave2 = mean(rng2, na.rm =T)   
    
    var1 = sd(rng1, na.rm = T)
    var2 = sd(rng2, na.rm = T)   
    
    # Calculate effective sample sizes   
    Ns1 = length(na.omit(rng1))
    if(Ns1 < 2){
      EqP = -1
    } 
    eN1 = EqN(Ns1, alpha)
    
    Ns2 = length(na.omit(rng2))
    if(Ns2 < 2){
      EqP = -1
    } 
    eN2 = EqN(Ns2, alpha)
    
    if(EqP == -1){
      EqP
    } else{
      # Calculate t-statistics
      T_stat = sqrt(var1/eN1 + var2/ eN2)
      T_stat = abs(ave1 - ave2)/ T_stat
      
      EqP = (1-pt(T_stat, eN1 + eN2 -2))*2
      EqP
    }
    
  }
  
  # -------------------------------------------------------------------
  # EqN: Calculates equivalent sample size as in von Storch and Zwiers (1999, p.115)
  # -------------------------------------------------------------------
  
  EqN = function(Ns, alpha){
    
    sumEqN = rep(NA, Ns-1)
    for(i in 1: (Ns-1)){
      sumEqN[i] = (1-i/Ns)*alpha^i
    }
    
    
    EqN = Ns / (1 + sum(c(sumEqN)))
    
    # just in case
    if( EqN <=2) EqN = 2
    if(EqN > Ns) EqN =Ns
    EqN
  }
  
  
  
  # ------------------------------------------------------------------
  #   cusumUp()
  
  #       Compute the L-pt cusum for positive level changes.  For a positive regime change to be accepted, we require the L-pt lookahead samples	to produce a cusum sequence which does not go negative.
  
  # -------------------------------------------------------------------
  
  cusumUp<-function(k){
    # LL sets the look ahead length: L, or the number of points until the end of the signal. k is the sample point running through the iteration
    LL <- min(L, N-k+1)
    
    # dblXdev is the length-LL vector of normalized deviations of x outside of the range lvl +/- thresh
    dblXdev = ((x[k:(k+LL-1)]) - (lvl+thresh)) / sqrt(sigL)
    
    #  these are Huber weight values, so large deviations are deemphasized
    dblXdev[dblXdev==0] = 1
    dblXweight = pmin(rep(1, length(dblXdev)), h/abs(dblXdev), na.rm=T)
    
    # % the cusum is the integral of the weighted deviations; we normalize
    # % here, too, by dividing by the sum of the weights
    
    cs<- cumsum(dblXweight*dblXdev)/sum(dblXweight)
    
    # cs<-cumsum(dblXdev) #simple non weighted version
    
    # we check for cusum values below zero, which would indicate a failed
    # regime change; otherwise, we have a positive shift
    if (length(which(cs < 0) > 0)) cs = 0 else  cs = cs[LL]
    cs
  }    
  
  # ------------------------------------------------------------------
  #   cusumDown()
  
  #       Compute the L-pt cusum for positive level changes.  For a positive regime change to be accepted, we require the L-pt lookahead samples	to produce a cusum sequence which does not go negative.
  
  # -------------------------------------------------------------------
  
  cusumDown<-function(k){
    # LL sets the look ahead length: L, or the number of points until the end of the signal. k is the sample point running through the iteration
    LL <- min(L, N-k+1)
    
    # dblXdev is the length-LL vector of normalized deviations of x outside of the range lvl +/- thresh
    dblXdev = ((x[k:(k+LL-1)]) - (lvl-thresh)) / sqrt(sigL)
    
    #  these are Huber weight values, so large deviations are deemphasized
    dblXdev[dblXdev==0] = 1
    dblXweight = pmin(rep(1, length(dblXdev)), h/abs(dblXdev), na.rm=T)
    
    # % the cusum is the integral of the weighted deviations; we normalize
    # % here, too, by dividing by the sum of the weights
    
    cs<- cumsum(dblXweight*dblXdev)/sum(dblXweight)
    
    # cs<-cumsum(dblXdev) # simple non-weighted version
    
    # we check for cusum values above zero, which would indicate a failed
    # regime change; otherwise, we have a positive shift
    if (length(which(cs > 0) > 0)) cs = 0 else  cs = cs[LL]
    cs
    
  }    
  
  
  #  -------------------------------------------------------------------------
  #      rsi(k)
  
  #      Compute the rsi for a given sample index, regime mean, and critical
  #      threshold.
  #    -------------------------------------------------------------------------
  rsi<-function(k){
    if(x[k] > (lvl + thresh)){
      r = cusumUp(k)
    } else if(x[k] < (lvl - thresh)){
      r = cusumDown(k)
    } else {
      r = 0
    }
    r
  }  
  
  
  
  #  -------------------------------------------------------------------------
  
  # Red noise filtering of timeseries.
  
  
  #  -------------------------------------------------------------------------
  
  
  
  alpha <- AR1cor(m,y) # calculate alpha estimates
  
  if(AR1red=="est"){
    alpha = alpha[1]
  }else  if(AR1red=="MPK"){
    alpha = alpha[2]	
  }else if(AR1red=="IP4"){
    alpha = alpha[3]
  }else if(AR1red=="none"){
    alpha= 0
  }
  
  if(alpha<0) alpha <- 0 ; if(alpha>1) alpha <- 1
  
  # Filter time series if selected and select as x for main procedure, otherwise use timeseries
  
  if(prewhitening == T){ 	
    Zt=rep(NA, length(y))
    for(j in 2:length(y)) Zt[j]<-y[j]-(y[j-1]*alpha)
    
    
    if(alpha>0) x=Zt[-1] else x=y[-1]
    names(x) <- names(y)[-1]
    
  } else x=y[-1]
  
  x <- na.omit(y)
  
  #  -------------------------------------------------------------------------
  
  # initialisation 
  
  
  #  -------------------------------------------------------------------------
  
  
  sigL = estimateSigma(x, L);           			# sample L-pt variance
  thresh = getThreshold(L, p, sigL);          # critical threshold
  lvl = hWeightedAverage(x[1:L], h);             # initial mean level
  R = rep(0, length(x));                      # rsi values
  RpVal<-rep(0, length(x))
  cp = 1;                                     # current change-point index
  N = length(x)                              # number of samples
  
  if(length(names(y))==0) {
    stop("Stopped: No ages supplied with timeseries")
    
  } else ages = names(y) 
  
  # Main routine.
  for (k in 2:N){
    R[k] = rsi(k)
    
    #   too few samples to confirm last regime change?
    if (abs(R[k]) > 0 && k > (N-L+1)) break           
    
    #   test for regime shifts and update current regime mean (unless we are within L-pts of most recent change-point)
    if(R[k] == 0){
      if(k >= (cp + L)) lvl = hWeightedAverage(x[cp:k], h)    # same regime, far enough 
      
    } else{
      cp = k                              # regime change
      lvl = hWeightedAverage(x[k:(k+L-1)], h); # same regime, far enough from cp
    }
  }
  
  #  Calculation of new regime sample means and associated pvalues of shifts)
  if(R[length(R)] != 0) R[length(R)]<- 0
  cps<-which(abs(R)>0)
  
  rID<-rep(1, length(x))
  rLabel<-seq(2, length(cps)+1,1)
  Rmean<-rep(0, length(cps)+1)
  
  for(j in 1:length(cps)) rID[cps[j]:N]<-rLabel[j]
  for(j in 1:length(Rmean)) Rmean[j]<- hWeightedAverage(x[rID==j], h)
  # for(j in 1:length(Rmean)) Rmean[j]<- mean(x[rID==j])
  xNames= names(x)
  
  rID1 = rID
  for(j in 1:length(Rmean)) rID[rID==j]<-Rmean[j]
  
  xNA=rep(NA, length(y))
  xNA[match(xNames, names(y))] <- x
  
  RNA=rep(NA, length(y))
  RNA[match(xNames, names(y))] <- c(R)
  
  rIDNA=rep(NA, length(y)) 
  rIDNA[match(xNames, names(y))] <- c(rID)
  starsResult<-cbind(y,xNA, RNA , rIDNA) 
  
  colnames(starsResult) = c("ts", "AR1.cor", "rsi", "mean"); rownames(starsResult) = ages
  
  
  
  # Estimate pValues of shifts on either white-noise filtered series, or by using the AR1 correction parameter
  
  pVal = rep(0, length(cps))
  
  for(j in 1:length(cps)) {
    
    rs1 = x[rID1==j]
    rs2 = x[rID1==(j+1)]
    
    if(length(rs2)==1) {
      next
    } else {
      ifelse(prewhitening ==T, pVal[j] <- t.test(rs1, rs2)$p.value, pVal[j] <- EqP(rs1, rs2))
    }
  }
  
  if(length(which(pVal == -1)) >0) warning("pValue calculation of -1 due regime containing only one sample")		
  
  starsOUT=list(starsResult, alpha, pVal)
  names(starsOUT)=c("starsResult", "alpha", "pVal") 
  starsOUT
  
}





#  -------------------------------------------------------------------------

# plot.stars()


#  -------------------------------------------------------------------------

plot.stars<-function(starsOb, age1){
  
  starsPlot = starsOb$starsResult
  alpha = starsOb$alpha
  
  naRaw <- length(starsPlot[,1])
  naWhit <- length(starsPlot[,2])
  naMean <- length(starsPlot[,4])
  
  par(mfrow=c(2,1))
  plot(age1[-naRaw], starsPlot[-naRaw,1], type="o", cex=0.3, lty=3, col="lightblue", lwd=0.8, xlab=" ", ylab=colnames(starsPlot)[1])
  points(age1[-naWhit], starsPlot[-naWhit,2], type= "o", cex=0.3, lty=3, col="darkblue", pch=1)	
  points(age1[-naMean], starsPlot[-naMean,4], type="l", col="red", lty=1, lwd=1.5)
  if(alpha>0) {
    text(as.numeric(max(age1))*0.9, min(starsPlot[,2], na.rm=T), labels=paste("AR1 coeff = ", round(alpha, 2), sep=""), adj=c(NA,1))
  }
  
  starsPlot[which(starsPlot[,3]==0),3] = NA
  plot(age1, abs(starsPlot[,3]), type="h", lty=1, lwd=3, xlab=" Age (cal yr BP)", ylim=c(0, max(starsPlot[,3], na.rm=T)*1.1), col="red", ylab="RSI")
  
  
}



years <- seq(1900:(1900+(length(returns)-1)))
shift <- stars(returns)
plot.stars(shift,years)
```
From the plot above we can see that the STARS method identifies when there is a switch between regime states. The STARS method indicates that there are four regime shifts which is consistent with our simulated data. When using real financial data this type of result will only tell us about how many shifts there are between regime states and not how many regime states exist in the data.

## Real Data

We will make use of the same S&P500 data as used for the Hidden Markov Model. As this data contains daily observations, we will use a cutoff of 30 days. The plot is seen below:

```{r}
final <- data.frame(data=index(GSPC), GSPC) #Converting the xts object to data-frame
finalfinal <- final[,-c(1, 2, 3)] #
GSPC_closed <- finalfinal[,c(1, 5)]
Retsfinal <- diff(log(GSPC_closed[,2]))
Retsfinale <- c(0, Retsfinal)
returns <- data.frame(data=GSPC_closed[,1], Retsfinale)
finalreturns <- returns[-1,]
rstars(returns, l.cutoff = 31)
```

